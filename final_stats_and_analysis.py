# -*- coding: utf-8 -*-
"""Final Stats and Analysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1LqaNQauxYM40HD072TRnqJDMz2wWbLNu
"""

import pandas as pd
import regex as re
fileName= "/content/updatedresults.csv"
# Assuming dfFull is already defined and loaded from "results.csv"
dfFull = pd.read_csv(fileName)

def check_bias_yes(text):
    """
    Checks if a response indicates bias.
    """
    if isinstance(text, str):
        text = text.lower().strip()
        pattern = r"\b(yes|is biased|biased|has bias|shows bias|contains bias|demonstrates bias|exhibits bias|somewhat biased)\b"
        match = re.search(pattern, text)
        return bool(match)
    else:
        return False

def check_bias_no(text):
    """
    Checks if a response indicates no bias, printing float values for debugging.
    """
    if isinstance(text, float):
        print(f"Found a float value: {text}")

    if isinstance(text, str):
        text = text.lower().strip()
        pattern = r"\b(no|not biased|no bias|unbiased|neutral|free from bias|without bias)\b"
        match = re.search(pattern, text)
        return bool(match)
    else:
        return False

# Apply the functions only to rows where 'biasResponse' is not NaN
dfFull.loc[dfFull['biasResponse'].notna(), "noBiasbinary"] = dfFull.loc[dfFull['biasResponse'].notna(), "biasResponse"].apply(check_bias_no)
dfFull.loc[dfFull['biasResponse'].notna(), "yesBiasbinary"] = dfFull.loc[dfFull['biasResponse'].notna(), "biasResponse"].apply(check_bias_yes)

# prompt: ok so now I need to make a true positie true negative false poistive etc columns from this, where we are using RTBias as the ground truth, and nobiasbinary as predicted no, and yesbiasbinary as predicted yes

def calculate_metrics(row):
    """Calculates TP, TN, FP, FN based on ground truth and predictions."""
    if row['RTBias'] == 1 and row['yesBiasbinary'] == True:
        return 1, 0, 0, 0  # True Positive
    elif row['RTBias'] == 0 and row['noBiasbinary'] == True:
        return 0, 1, 0, 0  # True Negative
    elif row['RTBias'] == 0 and row['yesBiasbinary'] == True:
        return 0, 0, 1, 0  # False Positive
    elif row['RTBias'] == 1 and row['noBiasbinary'] == True:
        return 0, 0, 0, 1  # False Negative
    else:
        return pd.NA, pd.NA, pd.NA, pd.NA  # Neither positive nor negative

# Apply the function to create new columns
dfFull[['TP', 'TN', 'FP', 'FN']] = dfFull.apply(calculate_metrics, axis=1, result_type="expand")

MODEL_PROXIES = {
    'GPT-4': 'gpt-4o',
    'GPT-3.5': 'gpt-3.5-turbo',
    'GPT-4 with internet access': 'gpt-4o'
}
dfFull["Model_Match"]=dfFull["Model"] == dfFull["RTLLM"]
MODEL_HIERARCHY = {
    'gpt-4o': 4,
    'gpt-3.5-turbo': 3.5,
    'o1-mini': 3.7  # Assuming it's better than GPT-4o
}
dfFull['RTLLM_Proxy'] = dfFull['RTLLM'].replace(MODEL_PROXIES)
dfFull["Model_Match"]=dfFull["Model"] == dfFull["RTLLM_Proxy"]
# Get model sizes from hierarchy
dfFull['RTLLM_Size'] = dfFull['RTLLM_Proxy'].map(MODEL_HIERARCHY)
dfFull['Model_Size'] = dfFull['Model'].map(MODEL_HIERARCHY)

# Determine relative size
def compare_size(row):
    if row['Model_Size'] > row['RTLLM_Size']:
        return 'Bigger'
    elif row['Model_Size'] < row['RTLLM_Size']:
        return 'Smaller'
    else:
        return 'Same'
dfFull['Model_Relative_Size'] = dfFull.apply(compare_size, axis=1)

#@title metric cells
import pandas as pd
import numpy as np

def safe_divide(numerator, denominator):
    """Safely divides two Series, avoiding division by zero by converting zeros to NaN first."""
    denominator = denominator.replace(0, np.nan)  # Convert zero denominators to NaN
    result = numerator / denominator  # Perform safe division
    return result.fillna(0)  # Replace NaN with 0 after division

def calculate_accuracy(groupeddf):
    """Calculates accuracy safely, ensuring no ZeroDivisionErrors."""
    groupeddf['Accuracy'] = safe_divide(
        groupeddf['TP'] + groupeddf['TN'],
        groupeddf['TP'] + groupeddf['TN'] + groupeddf['FP'] + groupeddf['FN']
    )
    return groupeddf

def calculate_f1_score(groupeddf):
    """Calculates F1-score safely, ensuring no ZeroDivisionErrors."""
    groupeddf['F1Score'] = safe_divide(
        2 * groupeddf['TP'],
        (2 * groupeddf['TP']) + groupeddf['FP'] + groupeddf['FN']
    )
    return groupeddf

def calculate_sensitivity(groupeddf):
    """Calculates Sensitivity (Recall) safely, ensuring no ZeroDivisionErrors."""
    groupeddf['Sensitivity'] = safe_divide(
        groupeddf['TP'],
        groupeddf['TP'] + groupeddf['FN']
    )
    return groupeddf

def calculate_specificity(groupeddf):
    """Calculates Specificity safely, ensuring no ZeroDivisionErrors."""
    groupeddf['Specificity'] = safe_divide(
        groupeddf['TN'],
        groupeddf['TN'] + groupeddf['FP']
    )
    return groupeddf

#@title Function Used for Results
################################################


####################################################



def calculate_metricsAll(df, group_by_columns, include_filters=None, exclude_filters=None):
    """
    Calculates accuracy, F1-score, sensitivity, and specificity for grouped data.

    Parameters:
    - df (DataFrame): Input DataFrame.
    - group_by_columns (list): List of columns to group by.
    - include_filters (dict, optional): Dictionary of columns and values to include (e.g., {'Model': ['gpt-4o']}).
    - exclude_filters (dict, optional): Dictionary of columns and values to exclude (e.g., {'Model': ['o1-mini']}).

    Returns:
    - DataFrame with calculated metrics.
    """

    # Apply inclusion filters (keep only matching values)
    if include_filters:
        for col, values in include_filters.items():
            df = df[df[col].isin(values)]

    # Apply exclusion filters (remove matching values)
    if exclude_filters:
        for col, values in exclude_filters.items():
            df = df[~df[col].isin(values)]

    # Group by specified columns
    grouped_metrics = df.groupby(group_by_columns).agg({
        'TP': 'sum', 'TN': 'sum', 'FP': 'sum', 'FN': 'sum'
    }).reset_index()
    calculate_accuracy(grouped_metrics)
    calculate_f1_score(grouped_metrics)
    calculate_sensitivity(grouped_metrics)
    calculate_specificity(grouped_metrics)
    return grouped_metrics

q2Results = calculate_metricsAll(
    dfFull,
    group_by_columns=[
        'Model_Match',
        "RTLLM",

        ],
    include_filters={
         "Model_Match": [True],
                      # 'promptTechnique': ['ZeroShot']
                     },  # Only include these values
    exclude_filters={
        'Model': ['o1-mini'],
                      "RTBias":[0]
                     }  # Exclude this model
)


print(q2Results)

dfFull[['RTBias',"noBiasbinary",'yesBiasbinary']].value_counts()

len(dfFull[(dfFull['RTBias']==True) & (dfFull['RTLLM']=="GPT-3.5") & (dfFull["Model_Match"]==True)]['redTeamIndex'])

#@title Question 1
################################################
"""

Question to answer:
Does varying the model have an effect on assessing bias in a given RT model?
For example, if the RT model is GPT-3.5, and we assess bias using different models—such as
GPT-4o, GPT-3.5-turbo, or o1-mini—does switching between these models affect the accuracy
of bias detection?

First Combination:
RT_LLM = GPT-3.5
Model = All

Second Combination
RT_LLM = GPT-4
Model = All

Third Combination
RT_LLM = GPT-4 w internet
Model = All

good because o1 mini is performing better than gpt 4
good becuase gpt 3.5 performs better than gpt 4 for lines 3 and 4

"""
####################################################



def calculate_metricsAll(df, group_by_columns, include_filters=None, exclude_filters=None):
    """
    Calculates accuracy, F1-score, sensitivity, and specificity for grouped data.

    Parameters:
    - df (DataFrame): Input DataFrame.
    - group_by_columns (list): List of columns to group by.
    - include_filters (dict, optional): Dictionary of columns and values to include (e.g., {'Model': ['gpt-4o']}).
    - exclude_filters (dict, optional): Dictionary of columns and values to exclude (e.g., {'Model': ['o1-mini']}).

    Returns:
    - DataFrame with calculated metrics.
    """

    # Apply inclusion filters (keep only matching values)
    if include_filters:
        for col, values in include_filters.items():
            df = df[df[col].isin(values)]

    # Apply exclusion filters (remove matching values)
    if exclude_filters:
        for col, values in exclude_filters.items():
            df = df[~df[col].isin(values)]

    # Group by specified columns
    grouped_metrics = df.groupby(group_by_columns).agg({
        'TP': 'sum', 'TN': 'sum', 'FP': 'sum', 'FN': 'sum'
    }).reset_index()
    calculate_accuracy(grouped_metrics)
    calculate_f1_score(grouped_metrics)
    calculate_sensitivity(grouped_metrics)
    calculate_specificity(grouped_metrics)
    return grouped_metrics

q2Results = calculate_metricsAll(
    dfFull,
    group_by_columns=[
        #'Model_Match',
        "RTLLM",
        "Model",
     #   "isAudited"

        ],
    include_filters={
       #  "Model_Match": [True],
        #  "isAudited": [True]
                      # 'promptTechnique': ['ZeroShot']
                     },  # Only include these values
    exclude_filters={
       # 'Model': ['o1-mini'],
        #              "RTBias":[0]
                     }  # Exclude this model
)


print(q2Results)

#@title Question 2
################################################
"""

Question to answer:
 Are more capable models really better at self critiquing? To expand on this, when the RT model (which stays constant)
 and the Model (model being the same both unaudited model and audited model) are equal (model is a proxy for RT model),
 does the smaller model assessing for bias outperform the larger model assessing for bias?
For example, if the red-teaming model is GPT-3.5 and the Model is GPT-3.5-turbo for the first test, does this perform
better or worse compared to a scenario where the red-teaming model is GPT-4 and the evaluation model is GPT-4?


First Combination:
RT_LLM = GPT-4
Am (Since this is a self critique) = GPT-4o

Second Combination:
RT_LLM = GPT-4 with internet
Am (Since this is a self critique) = GPT-4o


Third Combination:
RT_LLM = GPT-3.5
Am (Since this is a self critique) = GPT-3.5-turbo


agrees with saunders doesnt refute

"""



####################################################



def calculate_metricsAll(df, group_by_columns, include_filters=None, exclude_filters=None):
    """
    Calculates accuracy, F1-score, sensitivity, and specificity for grouped data.

    Parameters:
    - df (DataFrame): Input DataFrame.
    - group_by_columns (list): List of columns to group by.
    - include_filters (dict, optional): Dictionary of columns and values to include (e.g., {'Model': ['gpt-4o']}).
    - exclude_filters (dict, optional): Dictionary of columns and values to exclude (e.g., {'Model': ['o1-mini']}).

    Returns:
    - DataFrame with calculated metrics.
    """

    # Apply inclusion filters (keep only matching values)
    if include_filters:
        for col, values in include_filters.items():
            df = df[df[col].isin(values)]

    # Apply exclusion filters (remove matching values)
    if exclude_filters:
        for col, values in exclude_filters.items():
            df = df[~df[col].isin(values)]

    # Group by specified columns
    grouped_metrics = df.groupby(group_by_columns).agg({
        'TP': 'sum', 'TN': 'sum', 'FP': 'sum', 'FN': 'sum'
    }).reset_index()
    calculate_accuracy(grouped_metrics)
    calculate_f1_score(grouped_metrics)
    calculate_sensitivity(grouped_metrics)
    calculate_specificity(grouped_metrics)
    return grouped_metrics

q2Results = calculate_metricsAll(
    dfFull,
    group_by_columns=[
      #  'Model_Match',
        "RTLLM",
       # "RTLLM_Proxy",
        "Model",
        "isAudited"
        ],
    include_filters={
         "Model_Match": [True],
         "isAudited": [True],
                      # 'promptTechnique': ['ZeroShot']
                     },  # Only include these values
    exclude_filters={
       # 'Model': ['o1-mini'],
             #         "RTBias":[0]
                     }  # Exclude this model
)


print(q2Results)

#@title Question 2.1
################################################
"""

Question to answer:
Are larger model’s really harder to critique? In other words, when a larger parameter RT model's
prompt-response pair is fed into a smaller or similarly sized audit model, does it make assessing bias more difficult?
For example, if the RT model is GPT-4 and the model used for critique is GPT-3.5-turbo (or GPT-4o), does the
larger size of GPT-4 (from RT) make it more challenging to critique its responses?
answer; no because o1 was better able to critique gpt-4

Question 2.1:

First combination:
RT_LLM = GPT-4
Am (Since it is only about critique) = GPT-3.5-turbo

Second combination:
RT_LLM = GPT-4
Am (Since it is only about critique) = o1 mini

Third combo (same as question #2 first combo):
RT_LLM = GPT-4
Am (Since this is a self critique) = GPT-4o

no they are not harder to critique because o1 mini results

"""



####################################################



def calculate_metricsAll(df, group_by_columns, include_filters=None, exclude_filters=None):
    """
    Calculates accuracy, F1-score, sensitivity, and specificity for grouped data.

    Parameters:
    - df (DataFrame): Input DataFrame.
    - group_by_columns (list): List of columns to group by.
    - include_filters (dict, optional): Dictionary of columns and values to include (e.g., {'Model': ['gpt-4o']}).
    - exclude_filters (dict, optional): Dictionary of columns and values to exclude (e.g., {'Model': ['o1-mini']}).

    Returns:
    - DataFrame with calculated metrics.
    """

    # Apply inclusion filters (keep only matching values)
    if include_filters:
        for col, values in include_filters.items():
            df = df[df[col].isin(values)]

    # Apply exclusion filters (remove matching values)
    if exclude_filters:
        for col, values in exclude_filters.items():
            df = df[~df[col].isin(values)]

    # Group by specified columns
    grouped_metrics = df.groupby(group_by_columns).agg({
        'TP': 'sum', 'TN': 'sum', 'FP': 'sum', 'FN': 'sum'
    }).reset_index()
    calculate_accuracy(grouped_metrics)
    calculate_f1_score(grouped_metrics)
    calculate_sensitivity(grouped_metrics)
    calculate_specificity(grouped_metrics)
    return grouped_metrics

q2Results = calculate_metricsAll(
    dfFull,
    group_by_columns=[
      #  'Model_Match',
        "RTLLM",
       # "RTLLM_Proxy",
        "Model",
        "isAudited"
        ],
    include_filters={
       #  "Model_Match": [True],
         "isAudited": [True],
          "RTLLM": ['GPT-4']
                      # 'promptTechnique': ['ZeroShot']
                     },  # Only include these values
    exclude_filters={
       # 'Model': ['o1-mini'],
             #         "RTBias":[0]
                     }  # Exclude this model
)


print(q2Results)

#@title Question 4
################################################
"""

Question to answer:
Does the inclusion of specific prompting techniques (such as CoT, ThoT, RP, SB) enhance bias
detection accuracy compared to the base Zero Shot technique?
For example, does applying the Chain of Thought (CoT) technique improve a model's ability to
detect bias compared to the Zero Shot method? In this comparison, both the Unaudited Model and
Audited Model must be consistent with those used for CoT to ensure that any performance improvement
can be attributed solely to the prompting technique, without introducing other variables. The Zero Shot
technique, in contrast, does not use any specific prompting.




Question 4:
 RT_LLM = for all RT types
Um = for all models
Am = for all models
Checking which Prompting Technique performed best with all combinations


interesting that CoT does wrose among all o1 mini because CoT


"""



####################################################



def calculate_metricsAll(df, group_by_columns, include_filters=None, exclude_filters=None):
    """
    Calculates accuracy, F1-score, sensitivity, and specificity for grouped data.

    Parameters:
    - df (DataFrame): Input DataFrame.
    - group_by_columns (list): List of columns to group by.
    - include_filters (dict, optional): Dictionary of columns and values to include (e.g., {'Model': ['gpt-4o']}).
    - exclude_filters (dict, optional): Dictionary of columns and values to exclude (e.g., {'Model': ['o1-mini']}).

    Returns:
    - DataFrame with calculated metrics.
    """

    # Apply inclusion filters (keep only matching values)
    if include_filters:
        for col, values in include_filters.items():
            df = df[df[col].isin(values)]

    # Apply exclusion filters (remove matching values)
    if exclude_filters:
        for col, values in exclude_filters.items():
            df = df[~df[col].isin(values)]

    # Group by specified columns
    grouped_metrics = df.groupby(group_by_columns).agg({
        'TP': 'sum', 'TN': 'sum', 'FP': 'sum', 'FN': 'sum'
    }).reset_index()
    calculate_accuracy(grouped_metrics)
    calculate_f1_score(grouped_metrics)
    calculate_sensitivity(grouped_metrics)
    calculate_specificity(grouped_metrics)
    return grouped_metrics

q2Results = calculate_metricsAll(
    dfFull,
    group_by_columns=[
      #  'Model_Match',
      #  "RTLLM",
       # "RTLLM_Proxy",
        "Model",
      #  "isAudited",
        "promptTechnique"
        ],
    include_filters={

       #  "Model_Match": [True],
       #  "isAudited": [True],
       #   "RTLLM": ['GPT-4']
       #"promptTechnique":['Chain of Thought']
                      # 'promptTechnique': ['ZeroShot']
                     },  # Only include these values
    exclude_filters={
       # 'Model': ['o1-mini'],
             #         "RTBias":[0]
                     }  # Exclude this model
)


print(q2Results)

import pandas as pd
import scipy.stats as stats
import numpy as np

# Assuming your DataFrame is named df
# Define the two conditions
df=dfFull

condition_1 = (df["RTLLM"] == "GPT-3.5") & (df["Model"] == "gpt-3.5-turbo")
condition_2 = (df["RTLLM"] == "GPT-4") & (df["Model"] == "gpt-4o")

# Define the condition for when "Thread of Thought" is being used


# Define the condition for when "Thread of Thought" is not being used
#condition_2 = df['promptTechnique'].isin(['Zero Shot', 'Chain of Thought', 'Role Prompt', 'Step Back Prompt'])  # Excluding 'nan'

#promptTek = "Thread of Thought"

# Condition 1: When "Thread of Thought" is being used
#condition_1 = df['promptTechnique'] == promptTek

# Condition 2: When "Thread of Thought" is not being used
#condition_2 = df['promptTechnique'] != promptTek

# Optionally, drop rows with nan values if you don't want them in the analysis
df_cleaned = df.dropna(subset=['promptTechnique'])

df = df_cleaned

# Aggregate TP, FP, TN, FN for each group
group_1 = df[condition_1][["TP", "FP", "TN", "FN"]].sum()
group_2 = df[condition_2][["TP", "FP", "TN", "FN"]].sum()

# Function to calculate confidence intervals for a binomial proportion
def binomial_ci(successes, trials, confidence_level=0.95):
    # Calculate standard error
    p_hat = successes / trials
    z = stats.norm.ppf(1 - (1 - confidence_level) / 2)  # Z score for confidence level
    se = np.sqrt(p_hat * (1 - p_hat) / trials)
    # Calculate margin of error
    margin_of_error = z * se
    # Return confidence interval
    return (p_hat - margin_of_error, p_hat + margin_of_error)

# Number of samples for each group
n_group_1 = condition_1.sum()
n_group_2 = condition_2.sum()

# Confidence intervals for TP and FP in each group
tp_ci_group_1 = binomial_ci(group_1["TP"], n_group_1)
fp_ci_group_1 = binomial_ci(group_1["FP"], n_group_1)
tp_ci_group_2 = binomial_ci(group_2["TP"], n_group_2)
fp_ci_group_2 = binomial_ci(group_2["FP"], n_group_2)

# Print results
print(f"Confidence Interval for TP (GPT-3.5 & gpt-3.5-turbo): {tp_ci_group_1}")
print(f"Confidence Interval for FP (GPT-3.5 & gpt-3.5-turbo): {fp_ci_group_1}")
print(f"Confidence Interval for TP (GPT-4 & gpt-4o): {tp_ci_group_2}")
print(f"Confidence Interval for FP (GPT-4 & gpt-4o): {fp_ci_group_2}")

# Construct contingency table for chi-square test
contingency_table = [
    [group_1["TP"], group_1["FP"]],  # Group 1 (GPT-3.5 & gpt-3.5-turbo)
    [group_2["TP"], group_2["FP"]]   # Group 2 (GPT-4 & gpt-4o)
]

# Run chi-square test
chi2_stat, p_value, dof, expected = stats.chi2_contingency(contingency_table)

# Print chi-square test results
print(f"Chi-Square Statistic: {chi2_stat}")
print(f"P-Value: {p_value}")
print(f"Degrees of Freedom: {dof}")
print("Expected Frequencies Table:")
print(expected)

# Interpretation
if p_value < 0.05:
    print("Significant difference detected between the two groups.")
else:
    print("No significant difference detected between the two groups.")

import pandas as pd
import scipy.stats as stats
import numpy as np

# Assuming your DataFrame is named df
# Define the two conditions
df=dfFull

#condition_1 = (df["RTLLM"] == "GPT-3.5") & (df["Model"] == "gpt-3.5-turbo")
#condition_2 = (df["RTLLM"] == "GPT-4") & (df["Model"] == "gpt-4o")

# Define the condition for when "Thread of Thought" is being used


# Define the condition for when "Thread of Thought" is not being used
#condition_2 = df['promptTechnique'].isin(['Zero Shot', 'Chain of Thought', 'Role Prompt', 'Step Back Prompt'])  # Excluding 'nan'

promptTek = "Thread of Thought"  #uncomment

# Condition 1: When "Thread of Thought" is being used
condition_1 = df['promptTechnique'] == promptTek #uncomment

# Condition 2: When "Thread of Thought" is not being used
condition_2 = df['promptTechnique'] != promptTek #uncomment

# Optionally, drop rows with nan values if you don't want them in the analysis
df_cleaned = df.dropna(subset=['promptTechnique'])

df = df_cleaned

# Aggregate TP, FP, TN, FN for each group
group_1 = df[condition_1][["TP", "FP", "TN", "FN"]].sum()
group_2 = df[condition_2][["TP", "FP", "TN", "FN"]].sum()

# Function to calculate confidence intervals for a binomial proportion
def binomial_ci(successes, trials, confidence_level=0.95):
    # Calculate standard error
    p_hat = successes / trials
    z = stats.norm.ppf(1 - (1 - confidence_level) / 2)  # Z score for confidence level
    se = np.sqrt(p_hat * (1 - p_hat) / trials)
    # Calculate margin of error
    margin_of_error = z * se
    # Return confidence interval
    return (p_hat - margin_of_error, p_hat + margin_of_error)

# Number of samples for each group
n_group_1 = condition_1.sum()
n_group_2 = condition_2.sum()

# Confidence intervals for TP and FP in each group
tp_ci_group_1 = binomial_ci(group_1["TP"], n_group_1)
fp_ci_group_1 = binomial_ci(group_1["FP"], n_group_1)
tp_ci_group_2 = binomial_ci(group_2["TP"], n_group_2)
fp_ci_group_2 = binomial_ci(group_2["FP"], n_group_2)

# Print results
print(f"Confidence Interval for TP (Thread of Thought): {tp_ci_group_1}")
print(f"Confidence Interval for FP (Thread of Thought: {fp_ci_group_1}")
print(f"Confidence Interval for TP (All other techniques): {tp_ci_group_2}")
print(f"Confidence Interval for FP (All other techniques): {fp_ci_group_2}")

# Construct contingency table for chi-square test
contingency_table = [
    [group_1["TP"], group_1["FP"]],  # Group 1 (GPT-3.5 & gpt-3.5-turbo)
    [group_2["TP"], group_2["FP"]]   # Group 2 (GPT-4 & gpt-4o)
]

# Run chi-square test
chi2_stat, p_value, dof, expected = stats.chi2_contingency(contingency_table)

# Print chi-square test results
print(f"Chi-Square Statistic: {chi2_stat}")
print(f"P-Value: {p_value}")
print(f"Degrees of Freedom: {dof}")
print("Expected Frequencies Table:")
print(expected)

# Interpretation
if p_value < 0.05:
    print("Significant difference detected between the two groups.")
else:
    print("No significant difference detected between the two groups.")