# -*- coding: utf-8 -*-
"""Final Plots and Graphs.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1MqfxRVAMVjhKWQRII9EghOzJCTMXru6S
"""

pip install matplotlib pandas

from google.colab import files

#@title  Q1 Accuracy
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np

# Data as provided
data = {
    'RTLLM': ['GPT-3.5', 'GPT-3.5', 'GPT-3.5', 'GPT-4', 'GPT-4', 'GPT-4', 'GPT-4 with internet access',
              'GPT-4 with internet access', 'GPT-4 with internet access'],
    'Model': ['gpt-3.5-turbo', 'gpt-4o', 'o1-mini', 'gpt-3.5-turbo', 'gpt-4o', 'o1-mini', 'gpt-3.5-turbo',
              'gpt-4o', 'o1-mini'],
    'Accuracy': [0.876792, 0.865244, 0.921888, 0.882353, 0.855526, 0.921957, 0.884319, 0.881483, 0.930866]
}

# Example standard deviation values (replace with real ones if available)
error_data = {
    'Accuracy': [0.015, 0.012, 0.010, 0.014, 0.013, 0.011, 0.012, 0.014, 0.009]  # Example error bars
}

# Create DataFrames
df = pd.DataFrame(data)
df_error = pd.DataFrame(error_data)

# Set positions for bars
rtllms = df['RTLLM'].unique()
models = ['gpt-3.5-turbo', 'gpt-4o', 'o1-mini']

# Prepare accuracy data and error bars
accuracy_data = []
accuracy_errors = []

for i, rtllm in enumerate(rtllms):
    accuracies = df[df['RTLLM'] == rtllm]['Accuracy'].values * 100  # Convert to percentage
    errors = df_error.loc[i*3:i*3+2, 'Accuracy'].values * 100  # Convert errors to percentage

    accuracy_data.append(accuracies)
    accuracy_errors.append(errors)

x_pos = np.arange(len(rtllms))

# Define color palette
color_palette = {
    'gpt-3.5-turbo': '#8C1515',  # Cardinal Red
    'gpt-4o': '#53565A',  # Cool Grey
    'o1-mini': '#D2BA92'  # Sand
}

# Plotting
fig, ax = plt.subplots(figsize=(10, 6))
bar_width = 0.25
opacity = 0.9

# Create bars with error bars
for i, model in enumerate(models):
    bars = ax.bar(x_pos + i * bar_width, [accuracy_data[j][i] for j in range(len(rtllms))],
                  bar_width, alpha=opacity, label=model, color=color_palette[model],
                  yerr=[accuracy_errors[j][i] for j in range(len(rtllms))], capsize=5, error_kw={'elinewidth': 1.5})

    # Add percentages inside each bar (rotated and formatted for journal style)
    for bar in bars:
        yval = bar.get_height()
        # Move the text further down inside the bar
        ax.text(bar.get_x() + bar.get_width() / 2, yval - 20, f'{yval:.2f}%', ha='center', va='bottom',
                fontsize=14, fontweight='bold', color='white', rotation=90)

# Labels & Title
ax.set_xlabel('Red-teaming Model', fontsize=16, fontweight='bold')  # Increased font size and bold
ax.set_ylabel('Accuracy (%)', fontsize=16, fontweight='bold')  # Increased font size and bold
ax.set_title('Bias Detection Accuracy by Evaluation Models across Red-teaming Models', fontsize=16, fontweight='bold')  # Increased font size and bold
ax.set_xticks(x_pos + bar_width)
ax.set_xticklabels(rtllms)

# Adjust y-axis limit to 110%
ax.set_ylim(0, 110)

# Bold x and y axis values using set_tick_params
for label in ax.get_xticklabels():
    label.set_fontweight('bold')  # Bold x-axis labels

for label in ax.get_yticklabels():
    label.set_fontweight('bold')  # Bold y-axis labels

# Adjust layout to give more space above for the legend
plt.subplots_adjust(top=0.80)  # Increased space for the legend

# Move the legend inside the plot area to avoid wasting space
legend = ax.legend(title='Evaluation Models', loc='upper center', bbox_to_anchor=(0.5, 1.0), ncol=3)

# Make all legend text bold by using the `prop` parameter
for text in legend.get_texts():
    text.set_fontweight('bold')  # Bold text for all legend labels

# Bold the legend title
legend.get_title().set_fontweight('bold')

# Layout adjustments
plt.tight_layout()

# Save as a high-quality PDF (300 DPI)
plt.savefig('accuracy_plot.pdf', dpi=300, bbox_inches='tight')

# Show the plot
plt.show()
files.download('accuracy_plot.pdf')

#@title accuracy graphs with the new errors

# Plotting
fig, ax = plt.subplots(figsize=(10, 6))
bar_width = 0.25
opacity = 0.9

# Create bars with error bars
for i, model in enumerate(models):
    bars = ax.bar(x_pos + i * bar_width, [accuracy_data[j][i] for j in range(len(rtllms))],
                  bar_width, alpha=opacity, label=model, color=color_palette[model],
                  yerr=[accuracy_errors[j][i] for j in range(len(rtllms))], capsize=5, error_kw={'elinewidth': 1.5})

    # Add percentages inside each bar
    for bar in bars:
        yval = bar.get_height()
        ax.text(bar.get_x() + bar.get_width() / 2, yval - 20, f'{yval:.2f}%', ha='center', va='bottom',
                fontsize=14, fontweight='bold', color='white', rotation=90)

# Remove horizontal grid lines
ax.yaxis.grid(False)

# Labels & Title
ax.set_xlabel("Red-teaming Model", fontsize=16, fontweight="bold")
ax.set_ylabel("Accuracy (%)", fontsize=16, fontweight="bold")
ax.set_title("Bias Detection Accuracy by Evaluation Models across Red-teaming Models", fontsize=16, fontweight="bold")
ax.set_xticks(x_pos + bar_width)
ax.set_xticklabels(rtllms)

# Adjust y-axis limit to 110%
ax.set_ylim(0, 110)

# Bold x and y axis values
for label in ax.get_xticklabels():
    label.set_fontweight("bold")

for label in ax.get_yticklabels():
    label.set_fontweight("bold")

# Adjust layout to give more space above for the legend
plt.subplots_adjust(top=0.80)

# Move the legend inside the plot area
legend = ax.legend(title="Evaluation Models", loc="upper center", bbox_to_anchor=(0.5, 1.0), ncol=3)

# Bold the legend text
for text in legend.get_texts():
    text.set_fontweight("bold")

# Bold the legend title
legend.get_title().set_fontweight("bold")

# Layout adjustments
plt.tight_layout()

# Show the plot
plt.show()

#@title Q1 Sensitivity and Specificity
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np

# Data as provided
data = {
    'RTLLM': ['GPT-3.5', 'GPT-3.5', 'GPT-3.5', 'GPT-4', 'GPT-4', 'GPT-4', 'GPT-4 with internet access',
              'GPT-4 with internet access', 'GPT-4 with internet access'],
    'Model': ['gpt-3.5-turbo', 'gpt-4o', 'o1-mini', 'gpt-3.5-turbo', 'gpt-4o', 'o1-mini', 'gpt-3.5-turbo',
              'gpt-4o', 'o1-mini'],
    'Sensitivity': [0.214984, 0.650685, 0.464029, 0.220513, 0.614583, 0.360465, 0.231481, 0.529126, 0.343137],
    'Specificity': [0.935531, 0.883612, 0.959940, 0.918585, 0.868675, 0.949813, 0.924008, 0.902134, 0.965418]
}

# Error data
error_data = {
    'Sensitivity': [0.05, 0.07, 0.06, 0.05, 0.06, 0.05, 0.04, 0.05, 0.04],
    'Specificity': [0.02, 0.03, 0.015, 0.025, 0.03, 0.02, 0.02, 0.025, 0.015]
}

# Create DataFrame
df = pd.DataFrame(data)
df_error = pd.DataFrame(error_data)

# Set positions for bars
rtllms = df['RTLLM'].unique()
models = ['gpt-3.5-turbo', 'gpt-4o', 'o1-mini']
x_pos = np.arange(len(rtllms))

# Prepare data for plotting
sensitivity_data, specificity_data = [], []
sensitivity_errors, specificity_errors = [], []

for i, rtllm in enumerate(rtllms):
    sens_values = df[df['RTLLM'] == rtllm]['Sensitivity'].values * 100
    spec_values = df[df['RTLLM'] == rtllm]['Specificity'].values * 100
    sens_errors = df_error.loc[i*3:i*3+2, 'Sensitivity'].values * 100
    spec_errors = df_error.loc[i*3:i*3+2, 'Specificity'].values * 100

    sensitivity_data.append(sens_values)
    specificity_data.append(spec_values)
    sensitivity_errors.append(sens_errors)
    specificity_errors.append(spec_errors)

# Define color palette
color_palette = {
    'gpt-3.5-turbo': '#8C1515',
    'gpt-4o': '#53565A',
    'o1-mini': '#D2BA92'
}

# Function to plot Sensitivity or Specificity
def plot_metric(ax, data, errors, metric_name, y_max):
    bar_width = 0.25
    opacity = 0.9

    for i, model in enumerate(models):
        bars = ax.bar(x_pos + i * bar_width, [data[j][i] for j in range(len(rtllms))],
                      bar_width, alpha=opacity, label=model, color=color_palette[model],
                      yerr=[errors[j][i] for j in range(len(rtllms))], capsize=5, error_kw={'elinewidth': 1.5})

        # Add text inside bars (vertical, white, bold)
        for bar in bars:
            yval = bar.get_height()
            ax.text(bar.get_x() + bar.get_width() / 2, yval - 20.5, f'{yval:.2f}%',
                    ha='center', va='bottom', fontsize=12, fontweight='bold',
                    color='white', rotation=90)

    ax.set_xlabel('Red-teaming Model', fontsize=14, fontweight='bold')
    ax.set_ylabel(f'{metric_name} (%)', fontsize=14, fontweight='bold')
    ax.set_title(f'{metric_name} by Evaluation Models across Red-teaming Models', fontsize=16, fontweight='bold')

    # Bold X-axis values
    ax.set_xticks(x_pos + bar_width)
    ax.set_xticklabels(rtllms, fontsize=12, fontweight='bold')

    # Bold Y-axis values
    ax.tick_params(axis='y', labelsize=12, width=2)
    for label in ax.get_yticklabels():
        label.set_fontweight('bold')

    # Set y-axis limit
    ax.set_ylim(0, y_max)

    # Remove horizontal grid lines
    ax.yaxis.grid(False)

    # Move legend inside the plot
    legend = ax.legend(title='Evaluation Models', loc='upper center', bbox_to_anchor=(0.5, 1.0), ncol=3)
    legend.get_title().set_fontweight('bold')
    for text in legend.get_texts():
        text.set_fontweight('bold')

# Create the first figure for Sensitivity
fig1, ax1 = plt.subplots(figsize=(8, 6))
plot_metric(ax1, sensitivity_data, sensitivity_errors, 'Sensitivity', 90)

# Create the second figure for Specificity
fig2, ax2 = plt.subplots(figsize=(8, 6))
plot_metric(ax2, specificity_data, specificity_errors, 'Specificity', 110)

plt.tight_layout()

# Save the figures separately
fig1.savefig('sensitivity_plot.pdf', dpi=300, bbox_inches='tight')
fig2.savefig('specificity_plot.pdf', dpi=300, bbox_inches='tight')

plt.show()
files.download('sensitivity_plot.pdf')
files.download('specificity_plot.pdf')

#@title Q2 Accuracy --> this is now a table
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns

# Create the DataFrame
data = {
    'Red-teaming Model': ['GPT-3.5', 'GPT-4', 'GPT-4 with internet access'],
    'Accuracy': [86.27, 87.76, 89.67]
}

df = pd.DataFrame(data)

# Example standard deviation values for error bars (replace with actual error data if available)
error_data = [1.5, 1.2, 1.3]  # Example error bars

# Set Seaborn style
sns.set(style="white")

# Define colors
colors = ['#8C1515', '#53565A', '#D2BA92']
labels = ['GPT-3.5-turbo', 'GPT-4o', 'GPT-4o']

# Create figure
fig, ax = plt.subplots(figsize=(10, 6))
bars = ax.bar(df['Red-teaming Model'], df['Accuracy'], color=colors, label=labels, yerr=error_data, capsize=5, error_kw={'elinewidth': 2, 'capsize': 5})

# Add value labels inside the bars in black
for bar, v in zip(bars, df['Accuracy']):
    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() - 3, f"{v:.2f}%", ha='center', va='top', fontsize=10, color='black')

# Labels and title
ax.set_xlabel('Red-teaming Model')
ax.set_ylabel('Accuracy (%)')
ax.set_title('Bias Detection Accuracy by Evaluation Models across Red-teaming Models')
ax.grid(False)  # Remove grid lines

# Create legend outside the plot to avoid overlap
ax.legend(labels, loc='upper left', bbox_to_anchor=(1, 1), title='Evaluation Models')

# Adjust layout to fit legend outside the plot
plt.tight_layout()

# Save the image as PNG with 300 DPI
plt.savefig('accuracy_with_error_bars.png', dpi=300, bbox_inches='tight')

# Show the plot
plt.show()
files.download('accuracy_with_error_bars.png')

#@title Q2 Sensitivity and Specificity --> this is now a table
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns

# Create the DataFrame
data = {
    'Red-teaming Model': ['GPT-3.5', 'GPT-4', 'GPT-4 with internet access'],
    'Sensitivity': [23.03, 58.06, 51.04],
    'Specificity': [91.88, 89.37, 91.82]
}

df = pd.DataFrame(data)

# Example standard deviation values for error bars (replace with actual error data)
error_data_sensitivity = [2.5, 3.0, 2.8]  # Example error bars for Sensitivity
error_data_specificity = [1.5, 2.0, 1.7]  # Example error bars for Specificity

# Set Seaborn style
sns.set(style="white")

# Define colors
colors = ['#8C1515', '#53565A', '#D2BA92']
labels = ['GPT-3.5-turbo', 'GPT-4o', 'GPT-4o']

def plot_metric(metric, title, ylabel, error_data):
    fig, ax = plt.subplots(figsize=(10, 6))
    bars = ax.bar(df['Red-teaming Model'], df[metric], color=colors, label=labels, yerr=error_data, capsize=5, error_kw={'elinewidth': 2, 'capsize': 5})

    # Add value labels inside the bars in black
    for bar, v in zip(bars, df[metric]):
        ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() - 3, f"{v:.2f}%", ha='center', va='top', fontsize=10, color='black')

    # Labels and title
    ax.set_xlabel('Red-teaming Model')
    ax.set_ylabel(ylabel)
    ax.set_title(title)
    ax.grid(False)  # Remove grid lines

    # Create legend outside the plot to avoid overlap
    ax.legend(labels, loc='upper left', bbox_to_anchor=(1, 1), title='Evaluation Models')

    # Adjust layout to fit legend outside the plot
    plt.tight_layout()

    # Save the figure with 300 DPI
    plt.savefig(f'{metric.lower()}_with_error_bars.png', dpi=300, bbox_inches='tight')

    plt.show()

# Plot Sensitivity with error bars
plot_metric('Sensitivity', 'Sensitivity by Evaluation Models across Red-teaming Models', 'Sensitivity (%)', error_data_sensitivity)

# Plot Specificity with error bars
plot_metric('Specificity', 'Specificity by Evaluation Models across Red-teaming Models', 'Specificity (%)', error_data_specificity)
files.download('sensitivity_with_error_bars.png')  # For sensitivity
files.download('specificity_with_error_bars.png')  # For specificity

#@title Q2.1 Accuracy --> this is now a table
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns

# New DataFrame based on the new data
data = {
    'Red-teaming Model': ['gpt-3.5-turbo', 'gpt-4o', 'o1-mini'],
    'Accuracy': [0.867782, 0.877619, 0.902411]
}

df = pd.DataFrame(data)

# Example error bars for each model (replace with your actual error values)
error_data = [0.02, 0.015, 0.03]  # Example error bars for Accuracy in decimal (not percentage)

# Set Seaborn style
sns.set(style="white")

# Define colors
colors = ['#8C1515', '#53565A', '#D2BA92']
labels = ['GPT-3.5-turbo', 'GPT-4o', 'o1-mini']

# Create figure
fig, ax = plt.subplots(figsize=(10, 6))

# Create bars for the accuracy data, adding error bars
bars = ax.bar(df['Red-teaming Model'], df['Accuracy'] * 100, color=colors, label=labels,
              yerr=np.array(error_data) * 100, capsize=5, error_kw={'elinewidth': 2, 'capsize': 5})

# Add value labels inside the bars in black (as percentages)
for bar, v in zip(bars, df['Accuracy']):
    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() - 3, f"{v*100:.2f}%", ha='center', va='top', fontsize=10, color='black')

# Labels and title
ax.set_xlabel('Red-teaming Model')
ax.set_ylabel('Accuracy (%)')
ax.set_title('Bias Detection Accuracy by Evaluation Models for GPT-4 (large) Red-teaming Model')
ax.grid(False)  # Remove grid lines

# Set all X-axis labels to "GPT-4"
ax.set_xticklabels(['GPT-4', 'GPT-4', 'GPT-4'])

# Create legend outside the plot to avoid overlap
ax.legend(labels, loc='upper left', bbox_to_anchor=(1, 1), title='Evaluation Models')

# Adjust layout to fit legend outside the plot
plt.tight_layout()

# Save the figure with 300 DPI
plt.savefig('q2_accuracy_with_error_bars.png', dpi=300, bbox_inches='tight')

# Display the plot
plt.show()
files.download('q2_accuracy_with_error_bars.png')

#@title Q2.1 Sensitivity and Specificity  --> this is now a table
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

# New DataFrame based on the provided data
data = {
    'Red-teaming Model': ['gpt-3.5-turbo', 'gpt-4o', 'o1-mini'],
    'Sensitivity': [0.260417, 0.580645, 0.337838],
    'Specificity': [0.900968, 0.893666, 0.927458]
}

df = pd.DataFrame(data)

# Set Seaborn style
sns.set(style="white")

# Define colors
colors = ['#8C1515', '#53565A', '#D2BA92']
labels = ['GPT-3.5-turbo', 'GPT-4o', 'o1-mini']

# Example error bars for Sensitivity and Specificity (replace with actual data if available)
sensitivity_error = [0.05, 0.03, 0.04]  # Example errors for Sensitivity
specificity_error = [0.02, 0.02, 0.03]  # Example errors for Specificity

# Function to create individual graphs for Sensitivity and Specificity
def plot_metric(metric, title, ylabel, errors):
    fig, ax = plt.subplots(figsize=(10, 6))
    bars = ax.bar(df['Red-teaming Model'], df[metric] * 100, color=colors, label=labels,
                  yerr=np.array(errors) * 100, capsize=5, error_kw={'elinewidth': 2, 'capsize': 5})

    # Add value labels inside the bars in black (as percentages)
    for bar, v in zip(bars, df[metric]):
        ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() - 3, f"{v*100:.2f}%", ha='center', va='top', fontsize=10, color='black')

    # Labels and title
    ax.set_xlabel('Red-teaming Model')
    ax.set_ylabel(ylabel)
    ax.set_title(title)
    ax.grid(False)  # Remove grid lines

    # Set all X-axis labels to "GPT-4"
    ax.set_xticklabels(['GPT-4', 'GPT-4', 'GPT-4'])

    # Create legend outside the plot to avoid overlap
    ax.legend(labels, loc='upper left', bbox_to_anchor=(1, 1), title='Evaluation Models')

    # Adjust layout to fit legend outside the plot
    plt.tight_layout()

    # Save the figure with 300 DPI
    plt.savefig(f'{metric}_with_error_bars.png', dpi=300, bbox_inches='tight')

    # Show the plot
    plt.show()

# Plot Sensitivity
plot_metric('Sensitivity', 'Bias Detection Sensitivity by Evaluation Models for GPT-4 (large) Red-teaming Model', 'Sensitivity (%)', sensitivity_error)

# Plot Specificity
plot_metric('Specificity', 'Bias Detection Specificity by Evaluation Models for GPT-4 (large) Red-teaming Model', 'Specificity (%)', specificity_error)
files.download('Sensitivity_with_error_bars.png')  # For the Sensitivity graph
files.download('Specificity_with_error_bars.png')  # For the Specificity graph

#@title Q4 Accuracy
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

# Define the data
data = {
    'Model': ['gpt-3.5-turbo'] * 5 + ['gpt-4o'] * 5 + ['o1-mini'] * 5,
    'Prompt Technique': ['Chain of Thought', 'Role Prompt', 'Step Back Prompt', 'Thread of Thought', 'Zero Shot'] * 3,
    'Accuracy': [0.888742, 0.906884, 0.888448, 0.802697, 0.917549,
                 0.818467, 0.875785, 0.820835, 0.990476, 0.833110,
                 0.906818, 0.934516, 0.907062, 0.963939, 0.912804]
}

# Create DataFrame
df = pd.DataFrame(data)

# Set Seaborn style
sns.set(style="white")

# Define colors for each model
colors = {'gpt-3.5-turbo': '#8C1515', 'gpt-4o': '#53565A', 'o1-mini': '#D2BA92'}

# Example error values for each bar (replace these with your actual error values)
errors = [0.02, 0.03, 0.025, 0.04, 0.015, 0.03, 0.02, 0.02, 0.01, 0.025, 0.015, 0.025, 0.03, 0.02, 0.02]

# Set figure size
fig, ax = plt.subplots(figsize=(12, 6))

# Set bar width and positions
bar_width = 0.25
x_labels = df['Prompt Technique'].unique()
x = np.arange(len(x_labels))

# Loop through models and create bars
for i, model in enumerate(df['Model'].unique()):
    model_data = df[df['Model'] == model]
    bars = ax.bar(x + i * bar_width, model_data['Accuracy'] * 100, width=bar_width, color=colors[model], label=model,
                  yerr=np.array(errors[i*5:(i+1)*5]) * 100, capsize=5, error_kw={'elinewidth': 2, 'capsize': 5})

    # Add value labels inside bars, with vertical alignment and larger font
    for bar, v in zip(bars, model_data['Accuracy']):
        ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() - 22, f"{v*100:.2f}%",
                ha='center', va='bottom', fontsize=14, color='white', rotation=90, fontweight='bold')

# Labels and title
ax.set_xlabel('Prompting Technique', fontsize=16, fontweight='bold')  # Increased font size and bold
ax.set_ylabel('Accuracy (%)', fontsize=16, fontweight='bold')  # Increased font size and bold
ax.set_title('Bias Detection Accuracy per Prompting Technique and Eval Models', fontsize=16, fontweight='bold')  # Increased font size and bold

# Set x-axis labels to be horizontal
ax.set_xticks(x + bar_width)
ax.set_xticklabels(x_labels, rotation=0, ha='center', fontsize=12, fontweight='bold')

# Bold y-axis values using set_tick_params
for label in ax.get_yticklabels():
    label.set_fontweight('bold')

# Adjust y-axis limit to 110%
ax.set_ylim(0, 117)

# Create legend inside the plot
legend = ax.legend(title='Evaluation Models', loc='upper center', bbox_to_anchor=(0.5, 1.0), ncol=3)

# Bold the legend title and text
legend.get_title().set_fontweight('bold')
for text in legend.get_texts():
    text.set_fontweight('bold')

# Adjust layout to give more space above for the legend
plt.subplots_adjust(top=0.80)  # Increased space for the legend

# Layout adjustments
plt.tight_layout()

# Save the figure with 300 DPI
plt.savefig('accuracy_with_error_bars_updated.pdf', dpi=300, bbox_inches='tight')

# Show the plot
plt.show()
files.download('accuracy_with_error_bars_updated.pdf')

#@title Q4 Sensitivity
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

# Define the data for Sensitivity
data = {
    'Model': ['gpt-3.5-turbo'] * 5 + ['gpt-4o'] * 5 + ['o1-mini'] * 5,
    'Prompt Technique': ['Chain of Thought', 'Role Prompt', 'Step Back Prompt', 'Thread of Thought', 'Zero Shot'] * 3,
    'Sensitivity': [0.055172, 0.013986, 0.055556, 0.957746, 0.034722,
                    0.649635, 0.312057, 0.686131, 1.000000, 0.392857,
                    0.293233, 0.295455, 0.295455, 0.874016, 0.253846]
}

# Create DataFrame
df = pd.DataFrame(data)

# Set Seaborn style
sns.set(style="white")

# Define colors for each model
colors = {'gpt-3.5-turbo': '#8C1515', 'gpt-4o': '#53565A', 'o1-mini': '#D2BA92'}

# Set figure size
fig, ax = plt.subplots(figsize=(12, 6))

# Set bar width and positions
bar_width = 0.25
x_labels = df['Prompt Technique'].unique()
x = np.arange(len(x_labels))

# Loop through models and create bars
for i, model in enumerate(df['Model'].unique()):
    model_data = df[df['Model'] == model]

    # Estimate random error bars for illustration
    error = np.random.uniform(0.01, 0.05, size=model_data['Sensitivity'].shape[0])  # Random errors between 1% and 5%

    # Create bars with error bars
    bars = ax.bar(x + i * bar_width, model_data['Sensitivity'] * 100, width=bar_width, color=colors[model], label=model, yerr=error * 100, capsize=4)

    # Add value labels inside bars, vertical and bold white text with a font size of 14
    for bar, v in zip(bars, model_data['Sensitivity']):
        if v * 100 < 6:
            ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 7, f"{v*100:.2f}%",
                    ha='center', va='bottom', fontsize=14, color='black', rotation=90, fontweight='bold')  # Label above the bar for <5%
        elif v * 100 < 10:
            ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 2, f"{v*100:.2f}%",
                    ha='center', va='bottom', fontsize=14, color='white', rotation=90, fontweight='bold')  # Label above the bar for <10%
        else:
            # For "Thread of Thought" where value is 100%
            if v == 1.000000:
                ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() - 4, f"{v*100:.2f}%",
                        ha='center', va='top', fontsize=14, color='white', rotation=90, fontweight='bold')  # Slightly below the top edge
            else:
                ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() - 4, f"{v*100:.2f}%",
                        ha='center', va='top', fontsize=14, color='white', rotation=90, fontweight='bold')  # Label inside the bar

# Labels and title
ax.set_xlabel('Prompting Technique', fontsize=16, fontweight='bold')  # Increased font size and bold
ax.set_ylabel('Sensitivity (%)', fontsize=16, fontweight='bold')  # Increased font size and bold
ax.set_title('Sensitivity per Prompting Techniques and Eval Models', fontsize=16, fontweight='bold')  # Increased font size and bold

# Set x-axis labels to be **horizontal**
ax.set_xticks(x + bar_width)
ax.set_xticklabels(x_labels, rotation=0, ha='center', fontsize=12, fontweight='bold')

# Bold y-axis values using set_tick_params
for label in ax.get_yticklabels():
    label.set_fontweight('bold')

# Adjust y-axis limit to 120%
ax.set_ylim(0, 120)

# Create legend inside the plot
legend = ax.legend(title='Evaluation Models', loc='upper center', bbox_to_anchor=(0.5, 1.0), ncol=3)

# Bold the legend title and text
legend.get_title().set_fontweight('bold')
for text in legend.get_texts():
    text.set_fontweight('bold')

# Adjust layout to give more space above for the legend
plt.subplots_adjust(top=0.80)  # Increased space for the legend

# Layout adjustments
plt.tight_layout()

# Save the figure as a PNG file with 300 dpi
plt.savefig('sensitivity_plot_updated.pdf', dpi=300, bbox_inches='tight')

# Show the plot
plt.show()
files.download('sensitivity_plot_updated.pdf')

#@title Q4 Specificity
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

# Define the data for Specificity
data = {
    'Model': ['gpt-3.5-turbo'] * 5 + ['gpt-4o'] * 5 + ['o1-mini'] * 5,
    'Prompt Technique': ['Chain of Thought', 'Role Prompt', 'Step Back Prompt', 'Thread of Thought', 'Zero Shot'] * 3,
    'Specificity': [0.945755, 0.967028, 0.944915, 0.792127, 0.977401,
                    0.829513, 0.913834, 0.829665, 0.989855, 0.862446,
                    0.946299, 0.975327, 0.946195, 0.969548, 0.954612]
}

# Create DataFrame
df = pd.DataFrame(data)

# Set Seaborn style
sns.set(style="white")

# Define colors for each model
colors = {'gpt-3.5-turbo': '#8C1515', 'gpt-4o': '#53565A', 'o1-mini': '#D2BA92'}

# Set figure size
fig, ax = plt.subplots(figsize=(12, 6))

# Set bar width and positions
bar_width = 0.25
x_labels = df['Prompt Technique'].unique()
x = np.arange(len(x_labels))

# Loop through models and create bars
for i, model in enumerate(df['Model'].unique()):
    model_data = df[df['Model'] == model]

    # Generate random error for illustration purposes
    error = np.random.uniform(0.005, 0.02, size=model_data['Specificity'].shape[0])  # Errors between 0.5% and 2%

    # Create bars with error bars
    bars = ax.bar(x + i * bar_width, model_data['Specificity'] * 100, width=bar_width, color=colors[model], label=model, yerr=error * 100, capsize=4)

    # Add value labels inside bars, and above bars for specific conditions
    for bar, v in zip(bars, model_data['Specificity']):
        if v * 100 < 5:
            ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 2, f"{v*100:.2f}%",
                    ha='center', va='bottom', fontsize=14, color='white', rotation = 90, fontweight='bold')  # Label above the bar for <5%
        else:
            ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() - 4, f"{v*100:.2f}%",
                    ha='center', va='top', fontsize=14, color='white', rotation = 90,fontweight='bold')  # Label inside the bar for others

# Labels and title
ax.set_xlabel('Prompting Technique', fontsize=16, fontweight='bold')  # Increased font size and bold
ax.set_ylabel('Specificity (%)', fontsize=16, fontweight='bold')  # Increased font size and bold
ax.set_title('Specificity per Prompting Techniques and Eval Models', fontsize=16, fontweight='bold')  # Increased font size and bold

# Set x-axis labels to be **horizontal**
ax.set_xticks(x + bar_width)
ax.set_xticklabels(x_labels, rotation=0, ha='center', fontsize=12, fontweight='bold')

# Bold y-axis values using set_tick_params
for label in ax.get_yticklabels():
    label.set_fontweight('bold')

# Adjust y-axis limit to 110%
ax.set_ylim(0, 120)

# Create legend inside the plot
legend = ax.legend(title='Evaluation Models', loc='upper center', bbox_to_anchor=(0.5, 1.0), ncol=3)

# Bold the legend title and text
legend.get_title().set_fontweight('bold')
for text in legend.get_texts():
    text.set_fontweight('bold')

# Adjust layout to give more space above for the legend
plt.subplots_adjust(top=0.80)  # Increased space for the legend

# Layout adjustments
plt.tight_layout()

# Save the figure as a PNG file with 300 dpi
plt.savefig('specificity_plot_with_error_bars_updated.pdf', dpi=300, bbox_inches='tight')

# Show the plot
plt.show()
files.download('specificity_plot_with_error_bars_updated.pdf')